# model/tokenizer
repo_name : "facebook/opt-125m"
gradient_checkpointing: false
save_name: # CHANGE
bos_token_id: 1
eos_token_id: 2
pad_token_id: 1
use_cache: True
dropout: 0.0 # for pretraining 0 is good, for finetuning try 0.1+
vocab_size: 32000


# dataset
streaming: false
num_proc: 8
dataset_path: "nomic-ai/gpt4all-j-prompt-generations" # CHANGE
data_version: null
max_length: 256
input_column: "prompt"
output_column: "response"
remove_columns: ["source", "prompt"]
eos_token: null
n_samples: 100


# train dynamics
lr: 2.0e-5
min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
weight_decay: 0.0 # increase during finetuning.
eval_every: 500
eval_steps: 105
save_every: 500
log_grads_every: 100
checkpoint: null
lora: false
warmup_steps: 500
num_epochs: 2 
out_dir: './out'
eval_interval: 2048
log_interval: 512
eval_iters: 500
eval_only: False # if True, script exits right after the first eval
push_to_hub_every: 32
always_save_checkpoint: True # if True, always save a checkpoint after each eval
epochs: 100
gradient_accumulation_steps: 32 #128 # used to simulate larger batch sizes
train_batch_size: 1 # if gradient_accumulation_steps > 1, this is the micro-batch size
eval_batch_size: 8  # train_batch_size * 16
block_size: 256 #1024
decay_lr: True # whether to decay the learning rate
warmup_iters: 2000 # how many steps to warm up for
lr_decay_iters: 600000 # should be ~= max_iters per Chinchilla
device: "cuda"
device_type: 'cuda' 
scheduler: "cosine"


# logging
wandb: false
wandb_entity: # CHANGE
wandb_project_name: # CHANGE
wandb_log: False # disabled by default
wandb_project: 'owts_finetune'
wandb_run_name: 'abdul_finetune' # 'run' + str(time.time())
seed: 42

#Accelerate config
mixed_precision: "fp16"
zero_stage: 2
gradient_clipping: 1.0

# Optimizer
learning_rate: 6e-4 # max learning rate, set to about 2e-5 for finetuning.
max_iters: 2000000 # total number of training iterations
beta1: 0.9
beta2: 0.95
optimizer_name: Adam8bit"

# pytorch setting
compile: false # use PyTorch 2.0 to compile the model to be faster