# model/tokenizer
repo_name : "Gptfinetune0/mistral"
gradient_checkpointing: false
n_positions: 2048
sliding_window: 4096
intermediate_size: 6144
rope_theta: 10000
n_layer: 24 #28
n_head : 24 
n_embd: 1536 
bos_token_id: 1
eos_token_id: 2
pad_token_id: 1
n_key_value_heads: 8
use_cache: True
activation_function: "silu"
layer_norm_epsilon: 1e-6
window: 64
dropout: 0.0 # for pretraining 0 is good, for finetuning try 0.1+
vocab_size: 32000


# dataset
dataset: "openwebtext"
num_proc: 8
train_data_path: "./train.bin" # CHANGE
eval_data_path: "./val.bin"
data_version: null
max_length: 256


# train dynamics
min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
weight_decay: 1e-1 # increase during finetuning.
eval_every: 500
eval_steps: 105
save_every: 500
log_grads_every: 100
checkpoint: null
warmup_steps: 500
out_dir: './out'
eval_interval: 2048
log_interval: 512
eval_iters: 500
eval_only: False # if True, script exits right after the first eval
push_to_hub_every: 32
always_save_checkpoint: True # if True, always save a checkpoint after each eval
init_from: 'hub'
epochs: 100
gradient_accumulation_steps: 32 #128 # used to simulate larger batch sizes
train_batch_size: 2 # if gradient_accumulation_steps > 1, this is the micro-batch size
eval_batch_size: 8  # train_batch_size * 16
block_size: 256 #1024
decay_lr: True # whether to decay the learning rate
warmup_iters: 2000 # how many steps to warm up for
lr_decay_iters: 600000 # should be ~= max_iters per Chinchilla
device: "cuda"
device_type: 'cuda' 
scheduler: "cosine"

# logging
wandb_log: False # disabled by default
wandb_project: 'owts'
wandb_run_name: 'abdul_mistral' # 'run' + str(time.time())
seed: 42

#Accelerate config
mixed_precision: "fp16"
zero_stage: 2
gradient_clipping: 1.0

# Optimizer
learning_rate: 6e-4 # max learning rate, set to about 2e-5 for finetuning.
max_iters: 2000000 # total number of training iterations
beta1: 0.9
beta2: 0.95
optimizer_name: "Adam8bit"

# pytorch setting
compile: false # use PyTorch 2.0 to compile the model to be faster